{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623ef4ef",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:14:45.033201Z",
     "start_time": "2024-01-12T10:14:44.992309700Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from warnings import filterwarnings\n",
    "from torch import nn, optim\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = True\n",
    "from randaugment import RandAugment, ImageNetPolicy\n",
    "from torch.autograd import Variable\n",
    "import timm\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.data.dataset_factory import create_dataset\n",
    "from timm.data.mixup import Mixup\n",
    "import os\n",
    "filterwarnings('ignore')\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic' \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ada66b",
   "metadata": {},
   "source": [
    "## **function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfaea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def imshow(img):\n",
    "    # [C, H, W]를 [H, W, C]로 변경합니다.\n",
    "    img = img.transpose((1, 2, 0))\n",
    "    # 표준화된 데이터를 원본 범위로 되돌립니다.\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = img * std + mean\n",
    "    # 이미지의 범위가 [0, 1]이 되도록 클리핑합니다.\n",
    "    img = np.clip(img, 0, 1)\n",
    "    # 이미지를 표시합니다.\n",
    "    plt.imshow(img)\n",
    "\n",
    "def train_model(model, train_dataloader, optimizer, loss_func, lr_scheduler, epochs= 10, device = torch.device('cuda')):\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        current_lr = get_lr(optimizer)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch+1,epochs, current_lr))\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            image_tensors, true_labels = batch\n",
    "            image_tensors, true_labels = image_tensors.to(device), true_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            pred_labels = model(image_tensors)\n",
    "            loss = loss_func(pred_labels, true_labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f'Loss: {avg_loss:.4f}, Time: {((time.time()-start_time)/60):.6f}')\n",
    "        print('-------------------------------')\n",
    "\n",
    "# 테스트 데이터셋에 대한 모델의 정확도를 계산하는 함수\n",
    "def calculate_testset_accuracy(model, test_loader, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correction_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, true_labels in test_loader:\n",
    "            image_tensors = image_tensors.to(device)\n",
    "            true_labels = true_labels.to(device)\n",
    "\n",
    "            pred_labels = model(image_tensors)\n",
    "            _, preds = torch.max(pred_labels, 1)\n",
    "            correction_count += torch.sum(preds == true_labels.data)\n",
    "\n",
    "    # 전체 정확도 계산\n",
    "    accuracy = correction_count.double() / len(test_loader.dataset)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# 테스트 데이터셋에 대한 모델의 클래스별 정확도를 계산하는 함수\n",
    "def calculate_testset_accuracy_per_class(model, test_loader, num_classes, device = torch.device('cuda')):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    corrected_class = [0 for _ in range(num_classes)]\n",
    "    total_class = [0 for _ in range(num_classes)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, true_labels in test_loader:\n",
    "            image_tensors = image_tensors.to(device)\n",
    "            true_labels = true_labels.to(device)\n",
    "            pred_labels = model(image_tensors)\n",
    "            _, preds = torch.max(pred_labels, 1)\n",
    "\n",
    "            for true_label, pred_label in zip(true_labels, preds):\n",
    "                if true_label == pred_label:\n",
    "                    corrected_class[true_label] += 1\n",
    "                total_class[true_label] += 1\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        if total_class[i] == 0:\n",
    "            print(f'Accuracy of class {i} : N/A (No samples in test set)')\n",
    "        else:\n",
    "            accuracy = 100 * corrected_class[i] / total_class[i]\n",
    "            print(f'Accuracy of class {i} : {accuracy:.2f}% ({corrected_class[i]}/{total_class[i]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a17d0",
   "metadata": {},
   "source": [
    "## **make DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96641d23db68d891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:14:46.915138Z",
     "start_time": "2024-01-12T10:14:46.755750400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "augment_transform = create_transform(224, is_training=True, auto_augment='rand-m6-mstd0.5', mean =[0.49623227595753333,0.48377202969644434,0.39612923273387035], std=[0.21602388510121484,0.2127661699292398,0.2127661699292398] )\n",
    "train_dataset = create_dataset(name= '', root='C:/Users/VDRC/Desktop/pythonFolder/SK-Biology-Ai/data/image_dataset/train_image_set', transform = augment_transform)\n",
    "val_dataset = create_dataset(name= '', root='C:/Users/VDRC/Desktop/pythonFolder/SK-Biology-Ai/data/image_dataset/test_image_set', transform = create_transform(224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a08a0fdb546c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:14:47.848447100Z",
     "start_time": "2024-01-12T10:14:47.824510800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mixup 인자 설정\n",
    "mixup_args = {\n",
    "    'mixup_alpha': 1.,\n",
    "    'cutmix_alpha': 1.,\n",
    "    'prob': 1,\n",
    "    'switch_prob': 0.5,\n",
    "    'mode': 'batch',\n",
    "    'label_smoothing': 0.1,\n",
    "    'num_classes': 13\n",
    "}\n",
    "\n",
    "# Mixup 객체 생성\n",
    "mixup_params = Mixup(**mixup_args)\n",
    "\n",
    "class MixupDataLoader(DataLoader):\n",
    "    def __init__(self, *args, mixup_params, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mixup_fn = mixup_params\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "            mixed_inputs, mixed_targets = self.mixup_fn(inputs, targets)\n",
    "            yield mixed_inputs, mixed_targets            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f8677fa526c197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:14:48.354003100Z",
     "start_time": "2024-01-12T10:14:48.337048Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataloader = MixupDataLoader(train_dataset, mixup_params=mixup_params, batch_size=32, shuffle=True, num_workers=6)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b017d",
   "metadata": {},
   "source": [
    "## **set model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d88089f1132475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:14:51.266299Z",
     "start_time": "2024-01-12T10:14:49.911496900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d62fe6b6fe4c27899978f8d74eee41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/179M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'resnet101'\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=13)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6443adb6ee22fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:14:51.974652500Z",
     "start_time": "2024-01-12T10:14:51.965676900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = StepLR(opt, step_size=1, gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be44a5",
   "metadata": {},
   "source": [
    "## **train & test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa73a8e8924922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:19:45.404262400Z",
     "start_time": "2024-01-12T10:17:33.035865900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model(model, train_dataloader, opt, loss_func, lr_scheduler, epochs= 10, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_testset_accuracy(model, val_dataloader)\n",
    "calculate_testset_accuracy_per_class(model, val_dataloader, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cea11a83749010d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T10:11:06.401189600Z",
     "start_time": "2024-01-12T10:11:06.165695100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../weight/{model_name}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
